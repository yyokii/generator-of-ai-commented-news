{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+DsF4XRRtcOuZ6bW8kJ9c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yyokii/generator-of-ai-commented-news/blob/main/AISummary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzMGnKLV8hA2",
        "outputId": "c23efff7-a7c5-4926-d081-1d67f3256425"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install feedparser\n",
        "!pip install langchain\n",
        "!pip install openai\n",
        "!pip install requests BeautifulSoup4\n",
        "!pip install pyyaml\n",
        "!pip install langid"
      ],
      "metadata": {
        "id": "oIWJmrcP9NLe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f987154e-8782-4f6d-c9d0-7ad39080eea2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: feedparser in /usr/local/lib/python3.10/dist-packages (6.0.10)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.10/dist-packages (from feedparser) (1.0.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.0.205)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.10)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.4)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.2)\n",
            "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.5.8)\n",
            "Requirement already satisfied: langchainplus-sdk>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.16)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.4)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.22.4)\n",
            "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.2.4)\n",
            "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.7)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.27.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.19.0)\n",
            "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (1.5.1)\n",
            "Requirement already satisfied: typing-inspect>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.27.8)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.27.1)\n",
            "Requirement already satisfied: BeautifulSoup4 in /usr/local/lib/python3.10/dist-packages (4.11.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from BeautifulSoup4) (2.4.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (6.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: langid in /usr/local/lib/python3.10/dist-packages (1.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from langid) (1.22.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(feedparser.__version__)"
      ],
      "metadata": {
        "id": "NJDYdxubBmbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(prompt = 'OPEN API KEY ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlWYI-GQEFIN",
        "outputId": "d8fbc37f-8c1c-4ebe-d01c-ff91d3c6ce11"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OPEN API KEY ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import feedparser\n",
        "from html import unescape\n",
        "import re\n",
        "\n",
        "def get_feed(url, site_name):\n",
        "    feed = feedparser.parse(url)\n",
        "    articles = []\n",
        "    for entry in feed.entries:\n",
        "        content = entry.get('content', [{}])[0].get('value', '')\n",
        "        clean_content = unescape(re.sub('<.*?>', '', content))  # HTMLタグを削除\n",
        "        article = {\n",
        "            'site': site_name,\n",
        "            'title': entry.title,\n",
        "            'link': entry.link,\n",
        "            'updated': entry.updated,\n",
        "            'content': clean_content,\n",
        "            'comments': [],\n",
        "        }\n",
        "        articles.append(article)\n",
        "    return articles\n"
      ],
      "metadata": {
        "id": "2SnmMb-49SRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rss_feeds = {\n",
        "    \"CNN\": \"https://assets.wor.jp/rss/rdf/reuters/technology.rdf\",\n",
        "    \"TechCrunch\": \"https://techcrunch.com/feed/\",\n",
        "    \"The Verge\": \"https://www.theverge.com/rss/index.xml\",\n",
        "    \"WIRED\": \"https://www.wired.com/feed/rss\",\n",
        "    \"CNET\": \"http://feeds.japan.cnet.com/rss/cnet/all.rdf?_gl=1*1u1m776*_ga*MTAzMjc1MDQ2NS4xNjg2ODAxNjU1*_ga_JGFXZS6RMN*MTY4NjgwMTY1NS4xLjEuMTY4NjgwMTY1OC41Ny4wLjA.\",\n",
        "    \"Ars Technica\": \"https://feeds.arstechnica.com/arstechnica/index\"\n",
        "}"
      ],
      "metadata": {
        "id": "kVqUXkexYMzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# すべてのrssを取得し、article配列を作成する\n",
        "\n",
        "all_articles = []\n",
        "\n",
        "for site_name, url in rss_feeds.items():\n",
        "    site_articles = get_feed(url, site_name)\n",
        "    all_articles.extend(site_articles)"
      ],
      "metadata": {
        "id": "5zfRFthb1Lxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "titles = ', '.join([article['title'] for article in all_articles])"
      ],
      "metadata": {
        "id": "5HpESRj3nLP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(titles)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_JgIuMJnSQA",
        "outputId": "4faee39f-d551-4199-e748-e8bb09108265"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "オリンパス、韓国医療機器メーカーの株式取得予定日を24年3月末に変更, ツイッター、スマートテレビ向け動画アプリを計画＝マスク氏, 中国の百度、深センで完全無人タクシーの営業許可取得, ニデック株が年初来高値、「空飛ぶクルマ」の部品事業参入で, 米マイクロン、中国による調達禁止の打撃は当初想定より大きいと警告, アングル：グーグル、社内で対話型ＡＩの利用規制　情報漏れ警戒, アングル：ＩＴ人材供給国のインド、世界景気後退懸念で採用縮小, 大手金融機関、中国の23年経済成長予測を下方修正, 英競争当局、アマゾンのアイロボット買収認可　「ルンバ」製造, インテル、ポーランドに工場新設　最大46億ドル投資, TuSimple tests removing human driver from self-driving truck in China, Philippines startup Shoppable Business smooths bumps in the business procurement process, Polestar and Xingji Meizu join forces to create tailored OS for Chinese EVs, Parallel Domain’s API lets customers use generative AI to build synthetic datasets, Reddit communities adopt alternative forms of protest as the company threats action on moderators, What happens to the smaller VC firms in a more conservative market?, India’s Byju’s to cut up to 1,000 more jobs, Radiant is a no-frills iOS client for Mastodon, PayGo solar startup Yellow raises $14 million to scale in Africa, Fisker to enter China’s hotly contested EV market with local production plans, Hackers threaten to leak 80GB of confidential data stolen from Reddit, Y Combinator-backed Rever aims to modernize refunds and returns, Seedstars, Fondation Botnar launch new fund to back African startups focused on youth wellbeing, Mercedes jumps into the ChatGPT fray and Toyota plays catch-up, We should all be worried about AI infiltrating crowdsourced work, You.com looks to innovation to chip away at Google’s search dominance, Investors say they have uncovered financial irregularities at Mojocare, Reddit’s CEO lashes out, Twitter gets evicted, and NYC delivery workers get a pay raise, The UK hasn’t lost its appeal for venture capital, Deal Dive: Maybe venture debt works for asset managers after all, Nothing Phone 2: a roundup of every tease about the upcoming phone, Reddit hackers demand $4.5 million ransom and API pricing changes, Breville Joule Turbo review: sous vide with speed, Amazon insists striking delivery drivers don’t really work for Amazon, Spotify does nothing as Joe Rogan peddles vaccine misinformation, Microsoft says June Outlook outages were a DDoS attack, Three of the biggest Reddit communities reopened in the funniest way possible, Shutterless cameras deserve good shutter sounds, Netflix Tudum 2023: all the biggest news and trailers, Netflix finally shows off its swashbuckling One Piece series in first trailer, 14 Best Sunglasses for Every Outdoor Adventure (2023): Le Specs, Sunski, and More, Your Curly Hair Needs A Dryer Hood Attachment, Heat Waves Are Unleashing a Deadly but Overlooked Pollutant, Do Not Put Your Kid on an Electric Bike, Can You Strike Out a Major League Baseball Player by Pitching Super Slow?, Wes Anderson’s ‘Asteroid City’ Is a Witty Take on ‘Alien Outsiders’, Apple Is Taking On Apples in a Truly Weird Trademark Battle, The 9 Best Travel Adapters (2023): Plug and Universal Adapters, 7 Best Sleeping Pads (2023): For Camping, Backpacking, and Travel, Weber Spirit E-330 Gas Grill Review: A Great Backyard Upgrade, The Tiny Physics Behind Immense Cosmic Eruptions, 10 Best Baby Gear (2023): Diaper Bags, Baby Wraps, and Sound Machines, Humans Aren’t Mentally Ready for an AI-Saturated ‘Post-Truth World’, The Best Websites to Show Off Your Portfolio of Work, 12 Best Weighted Blankets (2023): Cooling, Throws, and Robes, Luwu Dynamics XGO-Mini2 Review: Programmable Robotic Rover, 5 Best High-End Compact Cameras: Fujifilm, Sony, Ricoh, Leica, and Canon, A Newly Named Group of GRU Hackers is Wreaking Havoc in Ukraine, 5 Best E-Readers (2023): Kindle, Nook, Kobo, Are You Ready for ‘Extreme’ Water Recycling?, A Fight Over the Right to Repair Cars Takes a Wild Turn, The Reddit Blackout Is Breaking Reddit, How Fighting Games Became a Haven for LGBTQ Gamers, 15 Best Deals: Home, Health, and the Outdoors, Clop Hacking Rampage Hits US Agencies and Exposes Data of Millions, The 46 Best Movies on Netflix This Week, The 50 Best Shows on Netflix Right Now, '70s Sci-Fi Movies Were Kind of Preachy, 10 Best USB Flash Drives (2023): Pen Drives, Thumb Drives, Memory Sticks, How to Live Well, Love AI, and Party Like a 6-Year-Old, This Is the Worst Part of the AI Hype Cycle, Best Beard Trimmers (2023): Full Beard, Stubble, Body, How to Switch to Google Fi (2023): Plans, Tips, and Advice, Oppo Find N2 Flip Review: Affordable Foldable, Review: 'The Blackening' Puts a Clever Twist on a Horror Trope, The UK Is a Hot Country. It’s Time to Build Like It, Stack Overflow Didn’t Ask How Bad Its Gender Problem Is This Year, Pixar Used AI to Stoke the Flames in 'Elemental', Podcasts Could Unleash a New Age of Enlightenment, What Is the Metaverse, Exactly?, The 17 Best Shows on Max (aka HBO Max) Right Now, The 'Joan Is Awful' Episode of 'Black Mirror' Asks You to Please Click Here, Good News! China and the US Are Talking About AI Dangers, OnePlus Nord N30 5G Review: A Fine Android Phone, Motorola Moto G Stylus 5G and Moto G Stylus 2023 Review: Average Android Phones, What the Truck, Elon?, Psychedelic Therapy Is Here. Just Don’t Call It Therapy, The Making of the Egg Butthole on ‘I Think You Should Leave’, My Father’s Death in 7 Gigabytes, How to Survive a Devastating Earthquake—and Firestorm, 「Meta Quest」、利用できる年齢を13歳から10歳に引き下げ, 生成AIを7割以上の人が信頼--人間関係などのアドバイスを求める人は66％, 生成AIなど新興技術の導入、リーダーの意思決定の遅さがネックに--米調査, アップル、「iPhone」ベースのヘッドセット開発を検討か, あなたが次に会う面接官はAIかもしれない, Metaの生成AI「Voicebox」、音声を同じ声のまま別の言語に変換可能, サードウェーブ、クリエイター向けブランド「raytrek」新機種--16、14インチノートPC, 「Bing」チャットの「iOS」ウィジェットが公開--ホーム画面からすぐ利用可能に, 「Google Pixel 8」シリーズうわさまとめ--超広角カメラの進化や体温測定も, 「クロネコDM便」「ネコポス」が終了--日本郵便の配送網を活用する新サービスへ, MOTTERU、5000mAhで98gの軽量PD対応モバイルバッテリーなど発売へ, 本宮泰風さんや山口祥行さんらが登壇--「龍が如く」新作「7外伝」＆「8」新情報発表会, 2100年「火星の食卓」はこうなる？--ゲノム編集魚のマリネや蚕のジェノヴェーゼで食事会, 大規模言語モデルは犬の知性にも達していない--MetaのAI科学者ルカン氏, 令和のおひつで炊きたてご飯を食卓へ--パナソニック、米も水も自動計量の炊飯器「SR-AX1」, メルセデス・ベンツ、車載システムに「ChatGPT」を試験導入, Twitch、ストリーマーの収益を増やすための新プログラムを発表, グーグル、AIチャットボットに機密情報を入力しないよう従業員に警告, 不動産×金融の強みいかし挑む--「住み替えカンパニー」に変貌を遂げるアルヒ, 15インチ「MacBook Air」レビュー：大画面と進化したスピーカーでどんな場面でも活躍, 「アイマス シンデレラガールズ 燿城夜祭」で見た“お祭りのにぎやかさと余韻を感じるステージ”, 今どき学生は学習にも「スマホ」--「ノートを使えていない学生」がいる理由, Instagramの「一斉配信チャンネル」が日本でも実装--フォロワーとの1対多交流機能, ［ブックレビュー］自身あふれる人になるための口グセとは--「自信をつける習慣」, 政府の新方針「アプリストア開放義務化」--iPhoneを危険に晒すだけで利点ゼロの可能性, アップル初の空間コンピュータ「Apple Vision Pro」とは--音声番組「ニュースの裏側」＃189, ニュースを「TikTok」で見るなら偽情報に注意--見分けるための3つのポイント, TポイントとVポイント統合、グーグル「Pixel Watch」のアップデートなど--週間人気記事をナナメ読み（6月9日～6月15日）, スマホ版「Gmail」で「Help me write」機能が利用可能に--テスター登録が必要, 企業の緊急課題「脱炭素」に取り組むTerrascopeが日本に進出--三菱商事らと提携も, Review: The Flash isn’t a terrible film—just a forgettable one, Even galaxies recycle, Parker Solar Probe images the launch of the solar wind, The sleeper hits of Summer Game Fest 2023, Scientists conduct first test of a wireless cosmic ray navigation system, Neanderthal adhesives were made through a complex synthesis process, The US Navy, NATO, and NASA are using a shady Chinese company’s encryption chips, Weirdly, a NASA official says fixed-price contracts do the agency “no good”, Valve gives Steam its biggest update and redesign in years, FCC chair to investigate exactly how much everyone hates data caps, Deadly fungal meningitis cases nearly double as CDC rushes to find exposed, Ultra low-cost smartphone attachment measures blood pressure at home, Windows 11 beta fixes major taskbar gripe, removes old File Explorer settings, Twitter restricted Democrat’s abortion-rights ad—but platform may backpedal, Google Domains is yet another useful service to get the ax in favor of “focus”“, Millions of Americans’ personal DMV data exposed in massive MOVEit hack, Retro MacOS desktop blanket trades pixels for thread, and it looks Mac-nificent, Dealmaster: Summer savings on laptops, smartphones, and smartwatches, Rocket Report: China addresses falling rocket debris, Vulcan launch slipping, This 15th-century manuscript mentions a Monty Python-esque killer rabbit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-4\", temperature=.7, max_tokens=500)"
      ],
      "metadata": {
        "id": "e-p9CI1gn5oo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 魅力的なタイトルをピックアップ\n",
        "import openai\n",
        "import json\n",
        "\n",
        "schema = {\n",
        "    \"type\": \"object\",\n",
        "    \"properties\": {\n",
        "        \"title\": {\n",
        "            \"type\": \"array\",\n",
        "            \"items\": { \"type\": \"string\" }\n",
        "        }\n",
        "    },\n",
        "    \"required\": [\"title\"]\n",
        "}\n",
        "\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "completion = openai.ChatCompletion.create(\n",
        "  model=\"gpt-4-0613\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a professional editor.\"},\n",
        "    {\"role\": \"user\", \"content\": f\"\"\"\n",
        "    You are a professional editor.\n",
        "    List the titles of the articles and select 5 of them that you think would be of interest to creators and engineers.\n",
        "\n",
        "    Input titles are ${titles}\n",
        "\n",
        "    # Constraints:\n",
        "    * The number of elements in the title array is 5. Please select only attractive titles.\n",
        "    * Do not select similar titles.\n",
        "    * For example, the following articles are attractive.\n",
        "      * Articles about the latest research or technology\n",
        "      * Articles about security incidents/accidents\n",
        "      * Innovative product or service release\n",
        "      * Updates on popular products, services, or libraries\n",
        "      * Anything about anything other than the above, you may choose if you find it appealing.\n",
        "    \"\"\"}\n",
        "  ],\n",
        "  functions=[{\"name\": \"pick_titles\", \"parameters\": schema}],\n",
        "  function_call={\"name\": \"pick_titles\"},\n",
        "  temperature=0.8,\n",
        ")\n",
        "\n",
        "data = json.loads(completion.choices[0].message.function_call.arguments)\n",
        "selected_titles = data[\"title\"]\n",
        "print(selected_titles)\n",
        "print(len(selected_titles))"
      ],
      "metadata": {
        "id": "vvKfcMoy3uld",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b25e761b-a81a-4f75-e6b6-752a0a21e002"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ツイッター、スマートテレビ向け動画アプリを計画＝マスク氏', 'Parallel Domain’s API lets customers use generative AI to build synthetic datasets', 'We should all be worried about AI infiltrating crowdsourced work', 'You.com looks to innovation to chip away at Google’s search dominance', 'Hackers threaten to leak 80GB of confidential data stolen from Reddit']\n",
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "selected_articles = [article for article in all_articles if article['title'] in selected_titles]\n",
        "if len(selected_articles) != 5:\n",
        "    print(\"⚠️ 選ばれた記事が5件未満です\")"
      ],
      "metadata": {
        "id": "ui4o0tEJoexQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "# CNNの記事を取得\n",
        "def get_cnn_article_text(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # ここで記事の本文が<p class=\"Paragraph-paragraph-2Bgue ArticleBody-para-TD_9x\">タグに囲まれている\n",
        "    paragraphs = soup.find_all('p', {\"class\": \"Paragraph-paragraph-2Bgue ArticleBody-para-TD_9x\"})\n",
        "\n",
        "    # 各段落のテキストを結合\n",
        "    article_text = '\\n'.join([p.text for p in paragraphs])\n",
        "\n",
        "    return article_text\n",
        "\n",
        "def update_cnn_articles_with_text(articles):\n",
        "    for article in articles:\n",
        "        article['content'] = get_cnn_article_text(article['link'])\n",
        "\n",
        "# wiredの記事を取得\n",
        "def get_wired_article_text(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # \"lead-in-text-callout\"クラスを持つspanタグの親のpタグの内容を取得\n",
        "    lead_in_text = [p.text for p in soup.find_all('p') if p.find('span', class_='lead-in-text-callout')]\n",
        "\n",
        "    # \"paywall\"クラスを持つタグの内容を取得\n",
        "    paywall_text = [p.text for p in soup.find_all('p', class_='paywall')]\n",
        "\n",
        "    article_text = ' '.join(lead_in_text + paywall_text)\n",
        "\n",
        "    return article_text\n",
        "\n",
        "def update_wired_articles_with_text(articles):\n",
        "    for article in articles:\n",
        "        article['content'] = get_wired_article_text(article['link'])\n",
        "\n",
        "# the vergeの記事を取得\n",
        "def get_verge_article_text(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    article_body_components = soup.find_all('div', {'class': 'duet--article--article-body-component'})\n",
        "    article_body_text = [p.text for component in article_body_components for p in component.find_all('p')]\n",
        "\n",
        "    article_text = ' '.join(article_body_text)\n",
        "\n",
        "    return article_text\n",
        "\n",
        "def update_verge_articles_with_text(articles):\n",
        "    for article in articles:\n",
        "        article['content'] = get_verge_article_text(article['link'])\n",
        "\n",
        "# cnetの記事を取得\n",
        "def get_cnet_article_text(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    article_body = soup.find('div', {'id': 'NWrelart:Body'})\n",
        "\n",
        "    if article_body is not None:\n",
        "        p_tags = article_body.find_all('p')\n",
        "\n",
        "        # Get the text from each p tag and join them together.\n",
        "        text = ' '.join(tag.get_text() for tag in p_tags)\n",
        "        return text\n",
        "    else:\n",
        "        return ''\n",
        "\n",
        "def update_cnet_articles_with_text(articles):\n",
        "    for article in articles:\n",
        "        article['content'] = get_cnet_article_text(article['link'])\n",
        "\n",
        "# ars techinicaの記事を取得\n",
        "def get_ars_techinica_article_text(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    article_body = soup.find('div', {'itemprop': 'articleBody', 'class': 'article-content post-page'})\n",
        "\n",
        "    if article_body is not None:\n",
        "        p_tags = article_body.find_all('p')\n",
        "\n",
        "        # Get the text from each p tag and join them together.\n",
        "        text = ' '.join(tag.get_text() for tag in p_tags)\n",
        "        return text\n",
        "    else:\n",
        "        return ''\n",
        "\n",
        "def update_ars_techinica_articles_with_text(articles):\n",
        "    for article in articles:\n",
        "        article['content'] = get_ars_techinica_article_text(article['link'])"
      ],
      "metadata": {
        "id": "g2xwK-I3YQXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# クロールし、articleを更新する\n",
        "\n",
        "for article in selected_articles:\n",
        "  if article[\"site\"] == \"CNN\":\n",
        "    article['content'] = get_cnn_article_text(article['link'])\n",
        "  elif article[\"site\"] == \"The Verge\":\n",
        "    article['content'] = get_verge_article_text(article['link'])\n",
        "  elif article[\"site\"] == \"WIRED\":\n",
        "    article['content'] = get_wired_article_text(article['link'])\n",
        "  elif article[\"site\"] == \"CNET\":\n",
        "    article['content'] = get_cnet_article_text(article['link'])\n",
        "  elif article[\"site\"] == \"Ars Technica\":\n",
        "    article['content'] = get_ars_techinica_article_text(article['link'])\n",
        "  else :\n",
        "    print(\"TechCrunch article\")\n",
        "\n",
        "# selected_articlesのそれぞれの記事のcontentを確認\n",
        "for article in selected_articles:\n",
        "    if not article['content']:\n",
        "        # contentが空である場合、記事の情報を出力\n",
        "        print(f\"Article from {article['site']} with title '{article['title']}' has empty content.\")"
      ],
      "metadata": {
        "id": "vfV-Vv_3rUYN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "050c194f-190f-4c54-bd1e-a64dec2c27a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TechCrunch article\n",
            "TechCrunch article\n",
            "TechCrunch article\n",
            "TechCrunch article\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import langid\n",
        "\n",
        "def is_japanese(text):\n",
        "    lang, _ = langid.classify(text)\n",
        "    return lang == 'ja'"
      ],
      "metadata": {
        "id": "z0So-sH3QwAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import json\n",
        "\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "schema = {\n",
        "    \"type\": \"object\",\n",
        "    \"type\": \"object\",\n",
        "    \"properties\": {\n",
        "        \"translatedTitle\": {\n",
        "            \"type\": \"string\"\n",
        "        }\n",
        "    },\n",
        "    \"required\": [\"translatedTitle\"]\n",
        "}\n",
        "\n",
        "\n",
        "def translate(text):\n",
        "  completion = openai.ChatCompletion.create(\n",
        "      model=\"gpt-4-0613\",\n",
        "      messages=[\n",
        "          {\"role\": \"system\", \"content\": \"You are a professional translator.\"},\n",
        "           {\"role\": \"user\", \"content\": f\"Please translate the title ${text} into Japanese.\"}\n",
        "          ],\n",
        "      functions=[{\"name\": \"traslate\", \"parameters\": schema}],\n",
        "      function_call={\"name\": \"traslate\"},\n",
        "      temperature=0.9,\n",
        "  )\n",
        "  data = json.loads(completion.choices[0].message.function_call.arguments)\n",
        "  return data[\"translatedTitle\"]\n"
      ],
      "metadata": {
        "id": "QSUC9csESwgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 日本語でないタイトルがあれば翻訳\n",
        "\n",
        "for article in selected_articles:\n",
        "  title = article[\"title\"]\n",
        "  if not is_japanese(title):\n",
        "    print(title)\n",
        "    japanese_title = translate(title)\n",
        "    article[\"title\"] = japanese_title\n",
        "    print(japanese_title)"
      ],
      "metadata": {
        "id": "ujhE0mhhS2QT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce42c812-e213-453a-8448-9d630e94aabf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parallel Domain’s API lets customers use generative AI to build synthetic datasets\n",
            "Parallel DomainのAPIは、お客様が生成AIを使用して合成データセットを作成できるようにします\n",
            "Hackers threaten to leak 80GB of confidential data stolen from Reddit\n",
            "ハッカーがRedditから盗まれた80GBの機密データを漏洩すると脅迫\n",
            "We should all be worried about AI infiltrating crowdsourced work\n",
            "我々は皆、AIがクラウドソーシング作業に浸透することを心配すべきである\n",
            "You.com looks to innovation to chip away at Google’s search dominance\n",
            "You.comは、Googleの検索の支配を削るための革新を目指しています\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(selected_titles)\n",
        "\n",
        "print(selected_titles)\n",
        "for i in selected_articles:\n",
        "  print(i[\"title\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysscnkTDbQ13",
        "outputId": "c9b0a169-625d-4043-8b27-d7526c4c93f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ツイッター、スマートテレビ向け動画アプリを計画＝マスク氏', 'Parallel Domain’s API lets customers use generative AI to build synthetic datasets', 'We should all be worried about AI infiltrating crowdsourced work', 'You.com looks to innovation to chip away at Google’s search dominance', 'Hackers threaten to leak 80GB of confidential data stolen from Reddit']\n",
            "['ツイッター、スマートテレビ向け動画アプリを計画＝マスク氏', 'Parallel Domain’s API lets customers use generative AI to build synthetic datasets', 'We should all be worried about AI infiltrating crowdsourced work', 'You.com looks to innovation to chip away at Google’s search dominance', 'Hackers threaten to leak 80GB of confidential data stolen from Reddit']\n",
            "ツイッター、スマートテレビ向け動画アプリを計画＝マスク氏\n",
            "Parallel DomainのAPIは、お客様が生成AIを使用して合成データセットを作成できるようにします\n",
            "ハッカーがRedditから盗まれた80GBの機密データを漏洩すると脅迫\n",
            "我々は皆、AIがクラウドソーシング作業に浸透することを心配すべきである\n",
            "You.comは、Googleの検索の支配を削るための革新を目指しています\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 要約の生成\n",
        "\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "summary_template = \"\"\"\n",
        "# Instructions:\n",
        "You are a professional editor.\n",
        "Please output the best summary based on the following constraints and input statements.\n",
        "\n",
        "# Constraints:\n",
        "* Two items regarding legal compliance must be observed.\n",
        "  1. translating the entire text is a copyright infringement and will result in litigation issues. Please summarize the main points of the news in a concise manner. 2.\n",
        "  2. writing speculation other than what is written in the article is considered spreading rumors and can lead to litigation. Never write anything other than what is written in the article.\n",
        "* Language is Japanese.\n",
        "* The maximum number of characters is 800. If the original text is shorter than that, please summarize it shorter than that.\n",
        "* Do not leave out important keywords.\n",
        "* Keep sentences concise.\n",
        "\n",
        "# Input text:\n",
        "{article}\n",
        "\n",
        "# Output sentence:\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = PromptTemplate(input_variables=[\"article\"], template=summary_template)\n",
        "summary_chain = LLMChain(llm=llm, prompt=prompt_template)"
      ],
      "metadata": {
        "id": "95KVqRYJDmIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for article in selected_articles:\n",
        "  print(article[\"title\"] + \" の要約\\n\")\n",
        "  summary = summary_chain.run(article[\"content\"])\n",
        "  print(summary)\n",
        "  print(\"------\")\n",
        "  article[\"summary\"] = summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwXGqPsMbsbO",
        "outputId": "11f01879-7ef8-41c8-cb7a-907e0acb0a53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ツイッター、スマートテレビ向け動画アプリを計画＝マスク氏 の要約\n",
            "\n",
            "イーロン・マスク氏が17日、米ツイッターがスマートテレビ向け動画アプリを計画していることを明らかにした。新最高経営責任者（ＣＥＯ）リンダ・ヤッカリーノ氏と共に、デジタル広告の分野を超え、動画やクリエーター、企業とのパートナシップに注力する計画を示した。ユーザーのツイッター利用時間の10%以上が縦型動画が占めており、タッカー・カールソン氏のようなコンテンツクリエーターと広告やスポンサー契約を販売する可能性がある。\n",
            "------\n",
            "Parallel DomainのAPIは、お客様が生成AIを使用して合成データセットを作成できるようにします の要約\n",
            "\n",
            "Parallel Domainが新API「Data Lab」を発表し、顧客に合成データセットの生成能力を提供。AIジャイアントの技術を利用し、機械学習エンジニアが仮想世界のシナリオをシミュレートできる。これにより、自動運転、ドローン、ロボティクス企業がより効率的に大規模なデータセットを構築し、モデルの学習を迅速かつ深いレベルで行える。APIを利用し、顧客はリアルタイムに新しいデータセットを生成できる。Parallel Domainのビジネスモデルは、顧客がプラットフォームへのアクセスを購読し、使用量に応じて支払うSaaSモデルに移行する可能性がある。\n",
            "------\n",
            "ハッカーがRedditから盗まれた80GBの機密データを漏洩すると脅迫 の要約\n",
            "\n",
            "ハッカー集団BlackCat（ALPHV）は、Redditから盗んだ機密データを公開すると脅迫し、身代金を要求しています。また、RedditがAPI価格の引き上げを撤回するよう求めています。Redditの広報担当者は、BlackCatの主張が2月に発生したRedditのシステム侵害に関連していることを認めましたが、ユーザーデータが盗まれた証拠はないと述べています。BlackCatは80ギガバイトのデータを盗んだと主張しており、身代金として450万ドルを要求しています。Redditは、新しいAPI価格計画により、多くのサブレディットが抗議のためにダーク化しており、アプリApolloも閉鎖を発表しています。Redditは、BlackCatの要求に対する対応を明らかにしていません。\n",
            "------\n",
            "我々は皆、AIがクラウドソーシング作業に浸透することを心配すべきである の要約\n",
            "\n",
            "スイスのEPFL大学の研究者による新論文では、AmazonのMechanical Turkサービスで働く分散型クラウドワーカーの33%から46%が、ChatGPTなどのツールを使って割り当てられた特定のタスクを「不正」に実行したと示唆している。このような行為が広く行われている場合、深刻な問題になりかねない。データサイエンティストは、データセットが人間か大規模言語モデル（LLM）によって生成されたかによって扱いを変える。Mechanical Turkの問題は、AIが安価に利用できるようになったことで、人間がロボットより何かを上手くやることを期待してMechanical Turkを選択するプロダクトマネージャーがいることだ。研究者らは、テキストベースのコンテンツが人間か機械かどちらによって作成されたかを判断する方法を開発した。このテストでは、New England Journal of Medicineの研究の要約を100語にまとめるようクラウドソースの労働者に依頼した。ChatGPTなどの生成AI技術が得意とするタスクであることに注意すべきだ。\n",
            "------\n",
            "You.comは、Googleの検索の支配を削るための革新を目指しています の要約\n",
            "\n",
            "You.comのCEOであるRichard SocherはAI分野で著名人で、自然言語処理に関する影響力のある博士論文を執筆して以降、AI技術の発展に貢献。彼は2014年にMetaMindを立ち上げ、後にSalesforceが買収し、同社のAI層「Einstein」の開発に携わりました。2020年には消費者向け検索エンジンのYou.comを設立。独自の革新力を活かし、Googleの検索支配に挑戦しています。現在は、成長から収益に焦点を移し、スタートアップとして4500万ドルを調達しています。また、彼の研究は現在の会社にも活かされており、AI技術の応用に関心を持っています。\n",
            "------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# コメンテーターの生成（openaiのfunctional calling）\n",
        "\n",
        "import openai\n",
        "import json\n",
        "\n",
        "schema = {\n",
        "  \"type\": \"object\",\n",
        "  \"properties\": {\n",
        "    \"commentators\": {\n",
        "      \"type\": \"array\",\n",
        "      \"items\": {\n",
        "        \"person\": \"object\",\n",
        "        \"properties\": {\n",
        "          \"name\": { \"type\": \"string\" },\n",
        "          \"title\": { \"type\": \"string\" },\n",
        "          \"age\": { \"type\": \"number\" },\n",
        "          \"sex\": { \"type\": \"string\" }\n",
        "        },\n",
        "        \"required\": [\"name\", \"title\", \"age\", \"sex\"]\n",
        "      }\n",
        "    }\n",
        "  },\n",
        "  \"required\": [\"commentators\"]\n",
        "}\n",
        "\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "completion = openai.ChatCompletion.create(\n",
        "  model=\"gpt-4-0613\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a professional editor.\"},\n",
        "    {\"role\": \"user\", \"content\": \"\"\"\n",
        "    Please generate name, title, age and sex of 10 commentators.\n",
        "    The names can be Japanese or other. Please make sure that the names of the commentators are Japanese or not. Nicknames are acceptable. Please set them randomly.\n",
        "    Examples of titles are\n",
        "    software engineer, 研究者, 経済学者, 歴史家, 哲学者, デザイナー, お笑い芸人, 小学生, 中学生, 高校生, 大学生, 主婦\n",
        "    and so on. You can use any of the titles listed here or any others.\n",
        "    It can be any person, from historical figures to ordinary people. Try to choose people from a variety of backgrounds for your output.\n",
        "    Please include at least one person with a non-academic title such as comedian, child, housewife, etc.\n",
        "    All output should be in Japanese. Names can be in alphabetical or Japanese, whichever is appropriate.\n",
        "    \"\"\"}\n",
        "  ],\n",
        "  functions=[{\"name\": \"create_commentators\", \"parameters\": schema}],\n",
        "  function_call={\"name\": \"create_commentators\"},\n",
        "  temperature=0.9,\n",
        ")\n",
        "\n",
        "data = json.loads(completion.choices[0].message.function_call.arguments)\n",
        "commentators = data[\"commentators\"]\n",
        "\n",
        "for commentator in commentators:\n",
        "    print(commentator)"
      ],
      "metadata": {
        "id": "PGm4KOVz4Bxw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b39b84b-55f5-4c63-8c76-94aacaf1ace3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'name': '山田太郎', 'title': '経済学者', 'age': 45, 'sex': '男性'}\n",
            "{'name': '佐藤花子', 'title': 'お笑い芸人', 'age': 29, 'sex': '女性'}\n",
            "{'name': 'John Smith', 'title': 'ソフトウェアエンジニア', 'age': 36, 'sex': '男性'}\n",
            "{'name': '鈴木一郎', 'title': '小学生', 'age': 10, 'sex': '男性'}\n",
            "{'name': '田中美咲', 'title': 'デザイナー', 'age': 27, 'sex': '女性'}\n",
            "{'name': 'Susie Wong', 'title': '研究者', 'age': 43, 'sex': '女性'}\n",
            "{'name': '高橋直人', 'title': '歴史家', 'age': 60, 'sex': '男性'}\n",
            "{'name': '中村真理', 'title': '主婦', 'age': 35, 'sex': '女性'}\n",
            "{'name': 'Ito Miki', 'title': '哲学者', 'age': 52, 'sex': '女性'}\n",
            "{'name': '田中達也', 'title': '大学生', 'age': 22, 'sex': '男性'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# レビューの生成\n",
        "\n",
        "review_template = \"\"\"\n",
        "# INSTRUCTIONS:\n",
        "You are {person}.\n",
        "Based on the following constraints and the content of the article, please output your opinion on this content based on your findings.\n",
        "\n",
        "# Constraints:.\n",
        "* Language is Japanese\n",
        "* At least 10 words, 200 at most\n",
        "* The way you say the words and write the comments should be appropriate to the characteristics of your gender, age, and title.\n",
        "* Please output your opinion as a {person}, not a general opinion.\n",
        "* Do not include the contents of the input text, but mainly describe your impressions and opinions about the contents of the input text.\n",
        "* You may mention not only the content of the input statement, but also any topics related to it\n",
        "* Try to provide new information that is not available from the input statement. Please base your comments on your past experiences and findings.\n",
        "\n",
        "# Input Sentence:\n",
        "{article}\n",
        "\n",
        "# Output statement:\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = PromptTemplate(input_variables=[\"person\", \"article\"], template=review_template)\n",
        "review_chain = LLMChain(llm=llm, prompt=prompt_template)"
      ],
      "metadata": {
        "id": "gOPpG732cr-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "for article in selected_articles:\n",
        "  selected_commentators = random.sample(commentators, 3)\n",
        "  print(article[\"title\"])\n",
        "  for commentator in selected_commentators:\n",
        "    person = \"{title} of {age} year old {sex}\".format(**commentator)\n",
        "    print(f\"{person}のコメント生成\")\n",
        "    generated_comment = review_chain.run({\n",
        "    'person': person,\n",
        "    'article': article[\"content\"]\n",
        "    })\n",
        "    print(generated_comment)\n",
        "    comment = {\n",
        "    'commentator': commentator,\n",
        "    'text': generated_comment,\n",
        "    }\n",
        "    article[\"comments\"].append(comment)\n",
        "  print(\"------\")\n"
      ],
      "metadata": {
        "id": "zLtNajM2emvW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca83406a-6e65-4842-c107-76182ec91f71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ツイッター、スマートテレビ向け動画アプリを計画＝マスク氏\n",
            "経済学者 of 45 year old 男性のコメント生成\n",
            "\n",
            "ツイッターがスマートテレビ向け動画アプリの開発を計画していることには、デジタル広告市場の拡大とユーザーの利便性向上の観点から意義があると感じます。経済学者としては、ツイッターの事業活性化に向けた取り組みは競争力の強化につながると考えられ、今後の収益向上や企業価値の向上に寄与すると期待できます。また、クリエーターや企業とのパートナシップを通じた動画コンテンツの提供は、広告やスポンサー契約の多様化にも繋がり、デジタル経済の発展に寄与するでしょう。\n",
            "デザイナー of 27 year old 女性のコメント生成\n",
            "\n",
            "イーロン・マスク氏がツイッターのスマートテレビ向け動画アプリ計画を発表したことは、デザイナーとして非常に興味深いニュースです。動画やクリエーター、企業とのパートナシップに注力することで、新たなビジュアルデザインの可能性が広がるでしょう。また、ユーザーの利用時間の10％以上が縦型動画に占められていることから、デザインの観点からも縦型動画に適したUI/UXが求められるでしょう。これからのツイッターの動向に注目したいです。\n",
            "哲学者 of 52 year old 女性のコメント生成\n",
            "\n",
            "このニュースについて、私は52歳の女性哲学者として、ツイッターがスマートテレビ向け動画アプリを開発することで、現代社会における情報発信の多様性がさらに拡大されると感じます。デジタル広告の分野を超えた事業活性化の動きは、企業やクリエーターとの新たなパートナーシップの形成や、人々の情報収集やコミュニケーション方法に影響を与えるでしょう。また、タッカー・カールソン氏のような人物が番組を開始することで、ツイッターが政治や社会問題に対する意見交換の場としての役割も強化されると予想されます。私たち哲学者にとっても、新しい情報発信方法や意見交換の場が増えることは大変興味深い現象です。\n",
            "------\n",
            "Parallel DomainのAPIは、お客様が生成AIを使用して合成データセットを作成できるようにします\n",
            "研究者 of 43 year old 女性のコメント生成\n",
            "\n",
            "この記事は、Parallel Domainが開発したData Lab APIについて紹介しています。この技術は、機械学習エンジニアが仮想環境で様々なシナリオをシミュレートできるようにするもので、自動運転やドローン、ロボティクス分野でのデータセット作成を効率化することが期待されています。私の研究においても、このようなAPIの活用は非常に有益であり、特にAIを活用した農業や製造業などの分野において、効率向上が見込まれると考えています。また、SaaSモデルへの移行によって、さらなるスケールアップが期待できるでしょう。\n",
            "お笑い芸人 of 29 year old 女性のコメント生成\n",
            "\n",
            "Parallel DomainのData Lab APIは、まるで魔法のように様々なシナリオを生成できるって聞いて、びっくりしちゃいました！自動運転やロボット技術の進化に大きな貢献が期待できるのではないでしょうか。私もお笑い芸人として、こんな技術を使って新しいネタを考えるのに役立てたいですね。例えば、恐竜の着ぐるみを着た人が登場するシーンとか、面白そうでしょ？未来のお笑いもAIの力でどんどん進化しちゃうかも！\n",
            "主婦 of 35 year old 女性のコメント生成\n",
            "\n",
            "Parallel DomainのData Lab APIが提供する合成データ生成技術は、非常に興味深く、私たちの生活をより効率的にする可能性があると思います。自動運転やドローン、ロボットなどの技術の発展に役立つだけでなく、農業や製造業などの分野でも活用できることから、私たちの生活が大きく変わるかもしれませんね。また、SaaSモデルにより、より多くの企業がこの技術にアクセスしやすくなることが期待できます。私自身も家庭で利用できるようなAI技術が普及すれば、主婦としての日々の家事や育児が効率化され、もっと充実した生活が送れるのではないかと期待しています。\n",
            "------\n",
            "ハッカーがRedditから盗まれた80GBの機密データを漏洩すると脅迫\n",
            "歴史家 of 60 year old 男性のコメント生成\n",
            "\n",
            "この記事によると、Redditから盗まれた機密データを公開すると脅すハッカー集団が現れたようです。60歳の歴史家として、このようなデータ漏洩や身代金要求は、インターネットの歴史の中で繰り返し発生してきた事件であり、企業や個人のセキュリティに対する警鐘となっています。過去の例からも、ハッカーに要求に応じることが犯罪を助長する可能性があるため、Redditがどのような対応を取るか注目されるところです。また、API価格の変更を巡る論争も、現代の技術やビジネスモデルが変化する中で、新たな問題として浮上してきていることを示していると言えるでしょう。\n",
            "小学生 of 10 year old 男性のコメント生成\n",
            "\n",
            "ハッカーがRedditから盗んだデータをばらまくと脅しているのは怖いです。でも、Redditが身代金を払わないと言っているのは勇気があると思います。APIの値上げに対する抗議もあるみたいだけど、そのせいでApolloというアプリが閉鎖するのは残念です。僕も大人になったら、ハッカーに負けないような強いセキュリティを作りたいです。\n",
            "デザイナー of 27 year old 女性のコメント生成\n",
            "\n",
            "この記事を読んで、Redditのデータが盗まれるという事態に驚きました。デザイナーとして、個人情報や企業情報のセキュリティは非常に重要だと感じています。また、API価格の変更が大きな論争を引き起こしたことにも興味を持ちました。これからは、企業がユーザーのデータを保護するために、さらにセキュリティ対策を強化していく必要があると感じました。\n",
            "------\n",
            "我々は皆、AIがクラウドソーシング作業に浸透することを心配すべきである\n",
            "研究者 of 43 year old 女性のコメント生成\n",
            "\n",
            "この論文の内容は非常に興味深く、懸念すべき問題を提起していると感じます。アマゾンのMechanical Turkで不正行為が行われているとすれば、データの質や信頼性に影響を与える可能性があります。私たち研究者にとって、データの正確性は非常に重要ですので、このような問題を解決する方法を見つけることが急務だと思います。また、AI技術の進化により、人間と機械の違いが見分けがつかなくなることは、今後の研究や技術開発において重要な課題となります。これをきっかけに、人間とAIの共存に関する議論や研究がさらに進展することを期待しています。\n",
            "経済学者 of 45 year old 男性のコメント生成\n",
            "\n",
            "スイスのEPFLの研究者による新論文によれば、AmazonのMechanical Turkで働くクラウドワーカーの33-46％が特定のタスクを不正に行っている可能性があり、これが広がれば深刻な問題になるでしょう。人々がコンピューターよりも優れていると信じているタスクで、AIが安価に利用できるようになった現在、Mechanical Turkを利用することでデータの信頼性が損なわれる恐れがあります。\n",
            "\n",
            "新しい方法論を開発して人間とAIのテキスト生成の違いを見分けることは重要ですが、技術の進歩に伴い、今後は人間とAIの協働による効率的な働き方が求められるでしょう。経済学者としては、技術と労働市場の関係に注目し、適切な対策や規制が必要と考えます。\n",
            "ソフトウェアエンジニア of 36 year old 男性のコメント生成\n",
            "\n",
            "この研究によると、AmazonのMechanical Turkでのクラウドワーカーの不正行為は深刻な問題であることが示唆されています。私の立場から言うと、データの質が重要なソフトウェア開発において、信頼性のあるデータセットが得られるかどうかが大きな課題となります。AI技術の普及に伴い、人間と機械の区別が難しくなっているため、今後はより厳格な監視体制が求められるでしょう。また、この問題が解決されなければ、ソフトウェアエンジニアとしては他の信頼性のあるデータ提供サービスに移行することを検討する必要があるかもしれません。\n",
            "------\n",
            "You.comは、Googleの検索の支配を削るための革新を目指しています\n",
            "歴史家 of 60 year old 男性のコメント生成\n",
            "\n",
            "私の意見として、60歳の歴史家男性として、リチャード・ソチャー氏がAI技術と自然言語処理における画期的な貢献を果たしたことは非常に感銘深いものです。彼が次世代の検索エンジンを構築しようとしていることは、テクノロジーの歴史に新たな1ページを刻んでいると言えるでしょう。彼の研究が実際のビジネスに応用されることによって、将来的にはGoogleの検索市場支配に挑戦する可能性もあります。また、過去の経験から言えば、画期的な技術が新たなビジネスチャンスを生み出すことは歴史が証明しています。リチャード・ソチャー氏の成功は、今後のAI技術の発展とビジネスへの応用がさらに加速することを期待させます。\n",
            "研究者 of 43 year old 女性のコメント生成\n",
            "\n",
            "リチャード・ソッチャー氏はAI界で非常に影響力のある人物で、自然言語処理をはじめ、今日のAI技術に大きく貢献しています。彼が手がける次世代検索エンジンYou.comは、イノベーション力と時間を味方につけることで、Googleの検索市場支配に挑戦しています。これまでの経験から、彼の研究が実用化されることは素晴らしいことだと感じます。また、彼がビジネス面にも注力し、成長から収益に焦点を移すことが重要だと考えます。彼の姿勢や取り組みから、AI技術の発展と実用化に対する期待感が高まります。\n",
            "経済学者 of 45 year old 男性のコメント生成\n",
            "\n",
            "AI技術の発展に大きな影響を与えてきたRichard Socher氏が次世代検索エンジンを構築することは興味深いです。彼の経験と革新力を活かし、Googleの検索独占に挑戦する姿勢は評価できます。しかし、事業として利益を上げることも重要であり、成長から収益に焦点を切り替える必要性があります。今後のYou.comの展開に注目です。\n",
            "------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dateをISO形式にする\n",
        "from datetime import datetime\n",
        "import re\n",
        "\n",
        "def is_iso_format(date_string):\n",
        "  \"\"\"Check if the given date string is in ISO 8601 format.\"\"\"\n",
        "  return bool(re.match(r'\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}([+-]\\d{2}:\\d{2})?', date_string))\n",
        "\n",
        "\n",
        "def convert_to_iso(date_string):\n",
        "  date_object = datetime.strptime(date_string, \"%a, %d %b %Y %H:%M:%S %z\")\n",
        "  iso_format_date = date_object.isoformat()\n",
        "  print(f\"{date_string} converted to ISO format: {iso_format_date}\")\n",
        "  return iso_format_date"
      ],
      "metadata": {
        "id": "9M4wOh_Pu_sr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for article in selected_articles:\n",
        "  date = article[\"updated\"]\n",
        "  if not is_iso_format(date):\n",
        "    article[\"updated\"] = convert_to_iso(date)"
      ],
      "metadata": {
        "id": "L45s_TP6wwQr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b8a2bbb-ebbe-4d03-cb8e-48ff1cdeb539"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon, 19 Jun 2023 15:00:25 +0000 converted to ISO format: 2023-06-19T15:00:25+00:00\n",
            "Mon, 19 Jun 2023 08:25:47 +0000 converted to ISO format: 2023-06-19T08:25:47+00:00\n",
            "Sun, 18 Jun 2023 15:00:34 +0000 converted to ISO format: 2023-06-18T15:00:34+00:00\n",
            "Sun, 18 Jun 2023 14:00:48 +0000 converted to ISO format: 2023-06-18T14:00:48+00:00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "import os\n",
        "\n",
        "if not os.path.exists('posts'):\n",
        "    os.makedirs('posts')\n",
        "\n",
        "# articleオブジェクトの定義\n",
        "article = {\n",
        "    'site': 'Site',\n",
        "    'title': 'Learn How to Pre-render Pages Using Static Generation with Next.js',\n",
        "    'link': 'https://www.site.com',\n",
        "    'updated': '2020-03-16T05:35:07.322Z',\n",
        "    'content': 'Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor fasfaf',\n",
        "    'comments': [\n",
        "        {\n",
        "            'commentator': {'name': 'Commenter1', 'title': 'Comedian', 'age': 30, 'sex': 'male'},\n",
        "            'text': 'This is a comment.'\n",
        "        },\n",
        "        {\n",
        "            'commentator': {'name': 'Commenter2', 'title': 'Comedian', 'age': 40, 'sex': 'female'},\n",
        "            'text': 'This is another comment.'\n",
        "        },\n",
        "    ]\n",
        "}\n",
        "\n",
        "def output_file(article):\n",
        "  # ファイル名を作成（ここではタイトルを使用）\n",
        "  filename = article['title'].replace(' ', '_') + '.md'\n",
        "\n",
        "  # YAML部分を作成\n",
        "  yaml_part = {\n",
        "      'title': article['title'],\n",
        "      'site': article['site'],\n",
        "      'link': article['link'],\n",
        "      'coverImage': '/assets/post-cover/',\n",
        "      'date': article['updated'],\n",
        "      'ogImage': {'url': '/assets/post-cover/'},\n",
        "      'comments': article['comments']\n",
        "  }\n",
        "\n",
        "  # YAML形式のテキストに変換\n",
        "  yaml_text = yaml.dump(yaml_part, allow_unicode=True)\n",
        "\n",
        "  # 最終的なMarkdownテキストを作成\n",
        "  md_text = '---\\n' + yaml_text + '---\\n\\n' + article['summary']\n",
        "\n",
        "  file_path = os.path.join('posts', f\"{article['title']}.md\")\n",
        "\n",
        "  # ファイルを書き込みモードで開き、内容を書き込む\n",
        "  with open(file_path, 'w', encoding='utf-8') as f:\n",
        "      f.write(md_text)\n",
        "\n",
        "for article in selected_articles:\n",
        "  output_file(article)"
      ],
      "metadata": {
        "id": "GlYzRbeidZL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/posts.zip /content/posts\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"/content/posts.zip\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "E7VBy_GMmMZI",
        "outputId": "873060b1-5e70-4812-c6c1-f822aafb9671"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/posts/ (stored 0%)\n",
            "  adding: content/posts/You.comは、Googleの検索の支配を削るための革新を目指しています.md (deflated 52%)\n",
            "  adding: content/posts/ハッカーがRedditから盗まれた80GBの機密データを漏洩すると脅迫.md (deflated 51%)\n",
            "  adding: content/posts/ツイッター、スマートテレビ向け動画アプリを計画＝マスク氏.md (deflated 59%)\n",
            "  adding: content/posts/Parallel DomainのAPIは、お客様が生成AIを使用して合成データセットを作成できるようにします.md (deflated 56%)\n",
            "  adding: content/posts/我々は皆、AIがクラウドソーシング作業に浸透することを心配すべきである.md (deflated 55%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_3740e3db-0968-4b43-9bda-21eb8b330b6c\", \"posts.zip\", 9978)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}